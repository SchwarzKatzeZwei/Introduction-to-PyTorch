{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# モデルのパラメータを最適化する\n",
    "\n",
    "モデルとデータができたので、今度はモデルのパラメータをデータに最適化することで、モデルのトレーニング、バリデーション、テストを行います。\n",
    "モデルの学習は反復プロセスです。各反復（*epoch*と呼ばれる）において、モデルは出力を推測し、その推測の誤差（*loss*）を計算し、（モジュールで見たように）パラメータに関する誤差の導関数を収集し、勾配降下法を用いてパラメータを**最適化**します。このプロセスのより詳細なウォークスルーについては、[backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)のビデオをご覧ください。\n",
    "\n",
    "## 前提となるコード\n",
    "\n",
    "前のモジュールの**Datasets & DataLoaders**と**Build Model**のコードを読み込みます。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ハイパーパラメーターの設定 \n",
    "\n",
    "ハイパーパラメータは、モデルの最適化プロセスをコントロールするための調整可能なパラメータです。\n",
    "ハイパーパラメータの値が異なると、モデルの学習や収束率に影響します。\n",
    "([続きを読む](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)ハイパーパラメータのチューニングについて)\n",
    "\n",
    "トレーニング用に以下のハイパーパラメータを定義しています。\n",
    "\n",
    "- **Number of Epochs** - データセットを反復処理する回数\n",
    "- **Batch Size** - 各エポックでモデルが見るデータサンプルの数。\n",
    "- **Learning Rate** - 各バッチ/エポックでモデルのパラメータをどの程度更新するか。小さな値では学習速度が遅く、大きな値では学習中に予測できない動作をする可能性があります。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 最適化ループの追加\n",
    "\n",
    "ハイパーパラメータを設定したら、最適化ループを使ってモデルのトレーニングと最適化を行います。\n",
    "最適化ループの各繰り返しを **エポック** と呼びます。\n",
    "\n",
    "各エポックは主に2つの部分で構成されています。\n",
    "\n",
    "- **Train Loop** - トレーニングデータセットを繰り返し、最適なパラメータに収束させる。\n",
    "- **Validation/Test Loop** - テストデータセットを繰り返し処理し、モデルの性能が向上しているかどうかを確認します。\n",
    "\n",
    "トレーニングループで使われる概念を簡単に理解しておきましょう。\n",
    "一足先に最適化ループの `full-impl-label` をご覧ください。\n",
    "\n",
    "### 損失関数の追加\n",
    "\n",
    "学習データを提示しても、学習していないネットワークは正しい答えを出さない可能性があります。\n",
    "**損失関数**は、得られた結果の目標値に対する非類似度を測定します。\n",
    "これが学習時に最小化したい損失関数です。損失を計算するために、与えられた入力を用いて予測を行います。\n",
    "損失を計算するために、与えられたデータサンプルの入力を用いて予測を行い、それを真のデータラベル値と比較します。\n",
    "\n",
    "一般的な損失関数には、回帰タスクのための[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Square Error)や\n",
    "分類のための[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)（負の対数尤度）があります。\n",
    "[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) は `nn.LogSoftmax` と `nn.NLLLoss` を組み合わせたものです。\n",
    "\n",
    "モデルの出力logitsを`nn.CrossEntropyLoss`に渡し、logitsを正規化して予測誤差を計算します。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 最適化パス\n",
    "\n",
    "最適化とは、各学習ステップでモデルの誤差を減らすためにモデルのパラメータを調整するプロセスです。**最適化アルゴリズム**は、このプロセスの実行方法を定義します（この例では、確率的勾配降下法を使用します）。\n",
    "すべての最適化ロジックは ``optimizer`` オブジェクトにカプセル化されています。ここでは、SGDオプティマイザを使用しています。\n",
    "また、PADAMやRMSPropなど多くの[異なるオプティマイザ](https://pytorch.org/docs/stable/optim.html)があり、様々な種類のモデルやデータに適したオプティマイザーがPyTorchにはあります。\n",
    "\n",
    "学習が必要なモデルのパラメータを登録し、学習率ハイパーパラメータを渡すことでオプティマイザを初期化します。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "トレーニングループの中では3つのステップで最適化が行われます。\n",
    "\n",
    "* モデルパラメータの勾配をリセットするために、`optimizer.zero_grad()`を呼び出します。勾配はデフォルトで加算されますが、二重にカウントされるのを防ぐために各反復で明示的にゼロにします。\n",
    "* 予測損失を `loss.backwards()` の呼び出しで逆伝播させます。PyTorchは各パラメータに対する損失の勾配を取得します。\n",
    "* 勾配が得られたら、``optimizer.step()``を呼び出し、バックワードパスで収集した勾配によってパラメータを調整します。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 完全な実装\n",
    "\n",
    "最適化されたコードをループする`train_loop`と、テストデータに対してモデルの性能を評価する`test_loop`を定義します。\n",
    "テストデータに対してモデルのパフォーマンスを評価します。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "損失関数とオプティマイザを初期化し，それを`train_loop`と`test_loop`に渡します．\n",
    "モデルの性能が向上していることを確認するために、自由にエポック数を増やしてみてください。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296848  [    0/60000]\n",
      "loss: 2.289259  [ 6400/60000]\n",
      "loss: 2.281900  [12800/60000]\n",
      "loss: 2.282959  [19200/60000]\n",
      "loss: 2.252857  [25600/60000]\n",
      "loss: 2.234976  [32000/60000]\n",
      "loss: 2.251398  [38400/60000]\n",
      "loss: 2.221005  [44800/60000]\n",
      "loss: 2.214228  [51200/60000]\n",
      "loss: 2.191263  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 0.034320 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.189986  [    0/60000]\n",
      "loss: 2.181706  [ 6400/60000]\n",
      "loss: 2.143917  [12800/60000]\n",
      "loss: 2.168897  [19200/60000]\n",
      "loss: 2.101498  [25600/60000]\n",
      "loss: 2.064258  [32000/60000]\n",
      "loss: 2.102571  [38400/60000]\n",
      "loss: 2.035742  [44800/60000]\n",
      "loss: 2.038806  [51200/60000]\n",
      "loss: 1.991416  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.031120 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.999564  [    0/60000]\n",
      "loss: 1.978428  [ 6400/60000]\n",
      "loss: 1.889222  [12800/60000]\n",
      "loss: 1.946574  [19200/60000]\n",
      "loss: 1.816259  [25600/60000]\n",
      "loss: 1.774364  [32000/60000]\n",
      "loss: 1.827039  [38400/60000]\n",
      "loss: 1.715685  [44800/60000]\n",
      "loss: 1.739120  [51200/60000]\n",
      "loss: 1.659977  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.025918 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.696460  [    0/60000]\n",
      "loss: 1.662493  [ 6400/60000]\n",
      "loss: 1.518355  [12800/60000]\n",
      "loss: 1.609363  [19200/60000]\n",
      "loss: 1.459378  [25600/60000]\n",
      "loss: 1.438016  [32000/60000]\n",
      "loss: 1.486842  [38400/60000]\n",
      "loss: 1.376064  [44800/60000]\n",
      "loss: 1.416229  [51200/60000]\n",
      "loss: 1.323993  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.020917 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.400691  [    0/60000]\n",
      "loss: 1.377833  [ 6400/60000]\n",
      "loss: 1.210558  [12800/60000]\n",
      "loss: 1.329195  [19200/60000]\n",
      "loss: 1.182971  [25600/60000]\n",
      "loss: 1.194446  [32000/60000]\n",
      "loss: 1.235942  [38400/60000]\n",
      "loss: 1.147679  [44800/60000]\n",
      "loss: 1.192476  [51200/60000]\n",
      "loss: 1.113779  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.017670 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.191872  [    0/60000]\n",
      "loss: 1.191463  [ 6400/60000]\n",
      "loss: 1.009081  [12800/60000]\n",
      "loss: 1.155736  [19200/60000]\n",
      "loss: 1.011865  [25600/60000]\n",
      "loss: 1.039380  [32000/60000]\n",
      "loss: 1.089919  [38400/60000]\n",
      "loss: 1.011740  [44800/60000]\n",
      "loss: 1.057435  [51200/60000]\n",
      "loss: 0.994939  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.015716 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.056690  [    0/60000]\n",
      "loss: 1.078196  [ 6400/60000]\n",
      "loss: 0.878853  [12800/60000]\n",
      "loss: 1.047621  [19200/60000]\n",
      "loss: 0.911527  [25600/60000]\n",
      "loss: 0.936715  [32000/60000]\n",
      "loss: 1.002484  [38400/60000]\n",
      "loss: 0.928287  [44800/60000]\n",
      "loss: 0.970827  [51200/60000]\n",
      "loss: 0.921237  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.014465 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.962976  [    0/60000]\n",
      "loss: 1.004469  [ 6400/60000]\n",
      "loss: 0.789602  [12800/60000]\n",
      "loss: 0.974927  [19200/60000]\n",
      "loss: 0.848195  [25600/60000]\n",
      "loss: 0.863395  [32000/60000]\n",
      "loss: 0.945009  [38400/60000]\n",
      "loss: 0.874562  [44800/60000]\n",
      "loss: 0.911398  [51200/60000]\n",
      "loss: 0.870404  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.013598 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.893500  [    0/60000]\n",
      "loss: 0.951547  [ 6400/60000]\n",
      "loss: 0.724712  [12800/60000]\n",
      "loss: 0.922530  [19200/60000]\n",
      "loss: 0.804778  [25600/60000]\n",
      "loss: 0.809102  [32000/60000]\n",
      "loss: 0.902590  [38400/60000]\n",
      "loss: 0.838222  [44800/60000]\n",
      "loss: 0.868197  [51200/60000]\n",
      "loss: 0.831999  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.012954 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.839018  [    0/60000]\n",
      "loss: 0.909499  [ 6400/60000]\n",
      "loss: 0.675476  [12800/60000]\n",
      "loss: 0.882459  [19200/60000]\n",
      "loss: 0.772217  [25600/60000]\n",
      "loss: 0.767312  [32000/60000]\n",
      "loss: 0.868268  [38400/60000]\n",
      "loss: 0.812103  [44800/60000]\n",
      "loss: 0.834862  [51200/60000]\n",
      "loss: 0.800832  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.012446 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "モデルが最初はあまり良くないことに気付いたかもしれません（それでいいのです！）。もっと多くの `epochs` でループを実行するか、`learning_rate` をより大きな数値に調整してみてください。また、私たちが選んだモデル構成が、この種の問題に最適なものではないかもしれません（そうではありません）。この後の講座では、視覚問題に有効なモデルの形状について、より深く掘り下げていきます。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 自分の知識を確認する\n",
    "\n",
    "1. モデルの最適化における損失関数の目的は何ですか？\n",
    "\n",
    "- 損失関数は、得られた結果と目標値の非類似性の度合いを測定します。\n",
    "  > 不正解\n",
    "- 損失関数の勾配は、学習時にオプティマイザが適切なパラメータを調整するのに役立ちます。\n",
    "  > 不正解\n",
    "- 損失関数は，学習時に最小化されるものである\n",
    "  > 不正解\n",
    "- 上記のすべてが当てはまる\n",
    "  > 正解"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "a34f0c579350e7a308be104477b2a7af43d707a05d583f6c54fb3f18cffd7bfe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}