{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ``torch.autograd`` による自動微分\n",
    "\n",
    "ニューラルネットワークを学習する際、最もよく使われるアルゴリズムは**back propagation**です。\n",
    "このアルゴリズムでは、パラメータ（モデルの重み）はこのアルゴリズムでは、パラメータ（モデルの重み）は、与えられたパラメータに対する損失関数の**勾配**に応じて調整されます。\n",
    "**gradient**に応じてパラメータ（モデルの重み）を調整します。\n",
    "\n",
    "この勾配を計算するために、PyTorchは`torch.autogradat`という微分エンジンを内蔵しています。\n",
    "このエンジンは、任意の勾配の自動計算をサポートします。\n",
    "\n",
    "入力 `x`、パラメータ `w` および `b` と、何らかの損失関数を持つ最も単純な1層のニューラルネットワークを考えてみましょう。\n",
    "これは次のようにPyTorchで定義できます。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## テンソルと関数と計算グラフ\n",
    "\n",
    "このコードは、以下の**計算グラフ**を定義しています。\n",
    "\n",
    "![損失の勾配を計算するために、2つのパラメータ'w'と'b'を持つ計算グラフを示す図](images/computational-graph.png)\n",
    "\n",
    "このネットワークでは、`w`と`b`は**パラメータ**であり、これを最適化する必要があります。\n",
    "そのため、これらの変数に対する損失関数の勾配を計算する必要があります。\n",
    "これを行うためにはテンソルの`requires_grad`プロパティを設定します。\n",
    "\n",
    "> **Note:** `requires_grad` の値は，テンソルの作成時に設定することもできますし、後で `x.requirees_grad_(True)` メソッドを使って設定することもできます。\n",
    "\n",
    "計算グラフを構築するためにテンソルに適用する関数は実際にはクラス `Function` のオブジェクトです。\n",
    "このオブジェクトは、関数を *forward* 方向に計算する方法と、その微分を *backward* 方向に微分計算する方法を知っています。\n",
    "逆伝搬関数への参照は、テンソルの `grad_fn` プロパティに格納されています。\n",
    "`Function`の詳細については[ドキュメント](https://pytorch.org/docs/stable/autograd.html#function)を参照してください。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x12bc2c8e0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x12bc2c5e0>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 勾配計算\n",
    "\n",
    "ニューラルネットワークのパラメータの重みを最適化するためにはパラメータに対する損失関数の導関数を計算する必要があります。\n",
    "具体的にはある固定値の`x`と`y`の下で$\\frac{\\partial loss}{\\partial w}$と$\\frac{\\partial loss}{\\partial b}$が必要です。\n",
    "これらの導関数を計算するために、`loss.backward()`を呼び出し、`w.grad`と`b.grad`から値を取得します。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0424, 0.0851, 0.3282],\n",
      "        [0.0424, 0.0851, 0.3282],\n",
      "        [0.0424, 0.0851, 0.3282],\n",
      "        [0.0424, 0.0851, 0.3282],\n",
      "        [0.0424, 0.0851, 0.3282]])\n",
      "tensor([0.0424, 0.0851, 0.3282])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Note: 計算グラフのリーフノードのうち`requires_grad` プロパティが `True` に設定されているノードに対してのみ`grad` プロパティを取得することができます。\n",
    "グラフ内の他のすべてのノードでは勾配を利用することはできません。さらにパフォーマンス上の理由から`backward`を用いた勾配計算は1つのグラフに対して1回しか行うことができません。\n",
    "もし同じグラフに対して複数回の`backward`呼び出しを行う必要がある場合には`retain_graph=True`を`backward`呼び出しに渡す必要があります。\n",
    "\n",
    "## 勾配トラッキングの無効化\n",
    "\n",
    "デフォルトでは`requires_grad=True`となっているすべてのテンソルはその計算履歴を追跡し、勾配計算をサポートします。\n",
    "しかし勾配計算が必要ない場合もあります。例えば、モデルを学習した後そのモデルをある入力データに適用したい場合、つまり例えばモデルをトレーニングした後、入力データに適用したい場合、つまり、ネットワークを介して*forward* 計算だけを行いたい場合などです。\n",
    "追跡計算を止めるには計算コードを`torch.no_grad()`ブロックで囲むことで計算を停止することができます。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "同じ結果を得るためのもう一つの方法は，テンソルに`detach()`メソッドを使うことです。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "勾配トラッキングを無効にしたい理由はいくつかあります。\n",
    "\n",
    "  - ニューラルネットワークのいくつかのパラメータを **凍結パラメータ** にする。これは[学習済みネットワークの微調整](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)に一般的なシナリオです。\n",
    "  - フォワードパスのみを行っている場合に**計算を高速化**する。なぜならばテンソルの計算は勾配を追跡しない方が効率的だからです。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "計算機上のグラフの詳細\n",
    "----------------------------\n",
    "概念的には、autogradはデータ（テンソル）と、実行されたすべての演算（およびその結果としての新しいテンソル）を[関数](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)からなる有向非巡回グラフ（DAG）に記録します。\n",
    "このDAGでは、葉が入力テンソル、根が出力テンソルです。このグラフを根から葉へとたどっていくと連鎖律を使って自動的に勾配を計算することができます。\n",
    "\n",
    "フォワードパスでは、autogradは2つのことを同時に行います。\n",
    "\n",
    "- 要求された演算を実行して結果のテンソルを計算する\n",
    "- DAGの中で演算の*勾配関数*を維持する。\n",
    "\n",
    "バックワードパスは、DAGのルートで`.backward()`が呼ばれたときに起動します。\n",
    "`autograd` は次に\n",
    "\n",
    "- 各 `.grad_fn` から勾配を計算します。\n",
    "- それらをそれぞれのテンソルの `.grad` 属性に蓄積する。\n",
    "- 連鎖律を利用して、リーフテンソルにまで伝搬します。\n",
    "\n",
    "**PyTorchではDAGは動的なものです**。\n",
    "\n",
    "重要なことは、グラフがゼロから再作成されるということです。\n",
    "`.backward()`を呼び出すたびに、autogradは新しいグラフの作成を開始します。\n",
    "これこそが、モデルで制御フロー文を使用できる理由です。\n",
    "必要に応じて、反復ごとに形状、サイズ、操作を変更することができます。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional reading: ヤコビアンとベクトル勾配の積\n",
    "\n",
    "多くの場合スカラーの損失関数があり、いくつかのパラメータに対する勾配を計算する必要があります。\n",
    "しかし，場合によっては出力関数が任意のベクトルである場合があります。\n",
    "このような場合、PyTorchは実際の勾配ではなく、いわゆる **ヤコビアン積** を計算することができます。\n",
    "\n",
    "ベクトル関数 $\\vec{y}=f(\\vec{x})$ に対してここで、$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ と $\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$ とすると、その勾配は\n",
    "次の式で与えられます *ヤコビアン行列*:\n",
    "\n",
    "$\\begin{align}\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\end{align}\\end{align}$\n",
    "\n",
    "ヤコビアン行列を計算する代わりに、PyTorchではヤコビアン積 **$v^T\\cdot J$** を計算します。\n",
    "これは引数に $v^T\\cdot J$ を指定して `backward` を呼び出すことで実現できます。\n",
    "$v$ のサイズは元のテンソルのサイズと同じでなければなりません．\n",
    "積を計算したい元のテンソルのサイズと同じである必要があります。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp + 1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "同じ引数で2回目に `backward` を呼び出すと、勾配の値が異なることに注意してください。\n",
    "これは以下の理由によるものです。\n",
    "`backward`伝搬を行う際、PyTorchは**グラデーションを蓄積**します。すなわち、計算グラフの全てのリーフノードの `grad` プロパティに計算された勾配の値が追加されます。\n",
    "もしも 適切な勾配を計算する必要があるならばプロパティをゼロにしておく必要があります。\n",
    "実際のトレーニングでは、*optimizer* がこれを行う手助けをしてくれます。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Note:** 以前はパラメータなしで `backward()` 関数を呼び出していました。\n",
    "これは`backward(torch.tensor(1.0))`を呼んでいるのと同じで、ニューラルネットワーク学習時の損失のようなスカラー値の関数の場合、勾配を計算するのに便利な方法です。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 自分の知識を確認する"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. torch.autogradエンジンの目的は、以下の通りです。\n",
    "\n",
    "- モデルの精度を自動的に評価\n",
    "  > 正しくは、モデルの最適化時に自動的に勾配を計算します。\n",
    "- モデルの内部レイヤー構造を自動的に最適化\n",
    "  > 正しくは、モデルの最適化時に自動的に勾配を計算します。\n",
    "- モデルの構築に使用するデータセットを自動的に最適化する\n",
    "  > 正しくは、モデルの最適化時に自動的に勾配を計算します。\n",
    "- モデルの最適化における勾配の自動計算\n",
    "  > 正解！"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "a34f0c579350e7a308be104477b2a7af43d707a05d583f6c54fb3f18cffd7bfe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}