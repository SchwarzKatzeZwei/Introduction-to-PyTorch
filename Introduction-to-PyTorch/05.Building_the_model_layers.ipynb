{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ニューラルネットワークの構築\n",
    "\n",
    "ニューラルネットワークは、データに対して演算を行う層やモジュールで構成されています。\n",
    "[torch.nn](https://pytorch.org/docs/stable/nn.html)名前空間は、独自のニューラルネットワークを構築するために必要なすべてのビルディングブロックを提供します。\n",
    "PyTorchの全てのモジュールは、[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)をサブクラス化しています。\n",
    "ニューラルネットワークは、モジュール自体が他のモジュール(層)で構成されています。この入れ子構造により\n",
    "複雑なアーキテクチャを簡単に構築・管理することができます。\n",
    "\n",
    "以下のセクションでは、FashionMNISTデータセットの画像を分類するためのニューラルネットワークを構築します。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## トレーニング用のハードウェアデバイスの取得\n",
    "\n",
    "GPUのようなハードウェア・アクセラレータでモデルを学習できるようにしたいと思います。\n",
    "[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html)が利用可能かどうかチェックしてみましょう。\n",
    "利用可能でなければCPUを使い続けることになります。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## クラスの定義\n",
    "\n",
    "ニューラルネットワークを定義するには、`nn.Module` をサブクラス化します。`__init__`でニューラルネットワークのレイヤーを初期化します。各 `nn.Module` サブクラスは入力データに対する操作を `forward` メソッドで実装しています。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`NeuralNetwork`のインスタンスを作成し、それを`device`に移動させて、その構造を表示します。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "モデルを使用するには、入力データを渡します。これにより、モデルの`forward`が実行されます。\n",
    "それらに加えていくつかの[バックグラウンド操作](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866)が実行されます。\n",
    "`model.forward()`を直接呼ばないでください!\n",
    "\n",
    "入力に対してモデルを呼び出すと、各クラスの生の予測値を持つ10次元のテンソルが返されます。\n",
    "これを `nn.Softmax` モジュールのインスタンスに渡すことで，予測密度を得ることができます．\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted class: tensor([4])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## モデルのレイヤー\n",
    "\n",
    "FashionMNISTモデルのレイヤーを分解してみましょう。説明するために以下を行います。\n",
    "サイズ**28x28**の3つの画像のサンプルミニバッチを取り、それをネットワークに通すとどうなるかを見てみましょう。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.Flatten\n",
    "\n",
    "[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)でレイヤーを初期化し、各2D 28x28画像を784ピクセル値の連続した配列に変換します（ミニバッチの次元（dim=0）は維持されます）。\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.Linear\n",
    "\n",
    "[linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)は保存されている重みとバイアスを使って、入力に線形変換を施すモジュールです。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "layer1 = nn.Linear(in_features=28 * 28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.ReLU\n",
    "\n",
    "非線形活性化は、モデルの入力と出力の間の複雑なマッピングを作成するものです。\n",
    "非線形活性化は、線形変換の後に適用され、非線形性を導入し、ニューラルネットワークが様々な現象を学習するのに役立ちます。\n",
    "\n",
    "このモデルでは、線形層の間に[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)を使用しています。\n",
    "しかし、モデルに非線形性を導入するための他のアクティベーションもあります。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before ReLU: tensor([[-0.0701, -0.3081,  0.3626, -0.0073,  0.3439, -0.1468,  0.2568, -0.5668,\n",
      "         -0.3017,  0.5958, -0.2099,  0.2591, -0.0570,  0.1017,  0.1859, -0.5327,\n",
      "          0.0203, -0.4696, -0.4637,  0.0412],\n",
      "        [ 0.1021, -0.0878,  0.4313, -0.1253,  0.3443,  0.1254,  0.1518, -0.1452,\n",
      "         -0.1431,  0.5728,  0.0506,  0.3015, -0.0988,  0.1512, -0.0285, -0.1421,\n",
      "         -0.1407, -0.3318, -0.3642,  0.1773],\n",
      "        [-0.3462, -0.4096,  0.5588,  0.0210,  0.4451, -0.0928,  0.4437, -0.1128,\n",
      "         -0.3589,  0.4629, -0.1584,  0.2374,  0.0369,  0.1840,  0.0866, -0.6190,\n",
      "          0.0120, -0.3414, -0.7449,  0.2643]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.3626, 0.0000, 0.3439, 0.0000, 0.2568, 0.0000, 0.0000,\n",
      "         0.5958, 0.0000, 0.2591, 0.0000, 0.1017, 0.1859, 0.0000, 0.0203, 0.0000,\n",
      "         0.0000, 0.0412],\n",
      "        [0.1021, 0.0000, 0.4313, 0.0000, 0.3443, 0.1254, 0.1518, 0.0000, 0.0000,\n",
      "         0.5728, 0.0506, 0.3015, 0.0000, 0.1512, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1773],\n",
      "        [0.0000, 0.0000, 0.5588, 0.0210, 0.4451, 0.0000, 0.4437, 0.0000, 0.0000,\n",
      "         0.4629, 0.0000, 0.2374, 0.0369, 0.1840, 0.0866, 0.0000, 0.0120, 0.0000,\n",
      "         0.0000, 0.2643]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.Sequential\n",
    "\n",
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)は、順序付けられたモジュールのコンテナです。データは定義されたとおりの順序ですべてのモジュールに渡されます。シーケンシャルコンテナを使って、`seq_modules`のような素早いネットワークを組むことができます。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### nn.Softmax\n",
    "\n",
    "ニューラルネットワークの最後の線形層は、`logits`（`-infty`, `infty` の生の値）を返して\n",
    "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)モジュールに渡されます。logitsは、各クラスに対するモデルの予測密度を表す値\\[0, 1\\]にスケーリングされます。パラメータ `dim` は，値の合計が1になるような次元を示します。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## モデルパラメータ\n",
    "\n",
    "ニューラルネットワークの多くの層は，*パラメータ化*されています．すなわち，学習時に最適化された重み，バイアスが関連付けられており、学習時に最適化されます。`nn.Module` をサブクラス化することでモデルオブジェクト内で定義されたすべてのフィールドを自動的に追跡し、モデルの `parameters()` や `named_parameters()` メソッドを使って、すべてのパラメータにアクセスできるようになります。\n",
    "\n",
    "この例では，各パラメータを繰り返し処理し，そのサイズと値のプレビューを表示しています。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 7.2443e-03, -2.1236e-02,  1.7604e-02,  ..., -1.0633e-02,\n",
      "         -1.5831e-02, -1.8122e-03],\n",
      "        [ 3.4512e-02, -2.5456e-02,  3.4360e-02,  ...,  6.9336e-03,\n",
      "          1.3214e-05,  6.4570e-03]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0254,  0.0086], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0265, -0.0267, -0.0338,  ..., -0.0372,  0.0042, -0.0288],\n",
      "        [-0.0150, -0.0190, -0.0026,  ..., -0.0379,  0.0133,  0.0265]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0035, -0.0263], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0034, -0.0126, -0.0193,  ..., -0.0342,  0.0267, -0.0371],\n",
      "        [ 0.0408, -0.0348, -0.0058,  ..., -0.0108, -0.0254,  0.0306]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0119, -0.0058], grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 知識の確認\n",
    "\n",
    "1. PyTorchの全てのニューラルネットワークモジュールの基本クラスはtorch.nn.Moduleです。\n",
    "\n",
    "* 真\n",
    "  > 正解!\n",
    "\n",
    "* 偽\n",
    "  > 間違っています。PyTorchの全てのニューラルネットワークモジュールの基本クラスはtorch.nn.Moduleです。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "a34f0c579350e7a308be104477b2a7af43d707a05d583f6c54fb3f18cffd7bfe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}